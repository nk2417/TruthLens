{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8fe9af81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb87910d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LOADING ADDITIONAL DATASETS FOR GENERALIZATION TESTING\n",
      "================================================================================\n",
      "\n",
      "Loading PolitiFact from: data_files/processed/politifact_combined.csv\n",
      "PolitiFact loaded: 624 articles\n",
      "Loading GossipCop from: data_files/processed/gossipcop_combined.csv\n",
      "GossipCop loaded: 14549 articles\n",
      "\n",
      "Dataset Summary:\n",
      "   Kaggle train: 30915 articles\n",
      "   Kaggle test: 7729 articles\n",
      "   PolitiFact: 624 articles\n",
      "   GossipCop: 14549 articles\n"
     ]
    }
   ],
   "source": [
    "from pipeline import load_kaggle, load_politifact, load_gossipcop, clean_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = load_kaggle()\n",
    "df = clean_dataset(df)\n",
    "df[\"text\"] = (df[\"title\"] + \" \" + df[\"text\"]).str.strip()\n",
    "df = df[[\"text\", \"label\"]]\n",
    "df_train, df_test = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "df_train.head()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING ADDITIONAL DATASETS FOR GENERALIZATION TESTING\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Load PolitiFact\n",
    "df_politifact = load_politifact()\n",
    "if df_politifact is not None:\n",
    "    df_politifact = clean_dataset(df_politifact)\n",
    "    df_politifact[\"text\"] = (df_politifact[\"title\"] + \" \" + df_politifact[\"text\"]).str.strip()\n",
    "    df_politifact = df_politifact[[\"text\", \"label\"]]\n",
    "    print(f\"PolitiFact loaded: {len(df_politifact)} articles\")\n",
    "else:\n",
    "    print(\"⚠️  Skipping PolitiFact (not available)\")\n",
    "\n",
    "# Load GossipCop\n",
    "df_gossipcop = load_gossipcop()\n",
    "if df_gossipcop is not None:\n",
    "    df_gossipcop = clean_dataset(df_gossipcop)\n",
    "    df_gossipcop[\"text\"] = (df_gossipcop[\"title\"] + \" \" + df_gossipcop[\"text\"]).str.strip()\n",
    "    df_gossipcop = df_gossipcop[[\"text\", \"label\"]]\n",
    "    print(f\"GossipCop loaded: {len(df_gossipcop)} articles\")\n",
    "else:\n",
    "    print(\"⚠️  Skipping GossipCop (not available)\")\n",
    "\n",
    "# Show dataset sizes\n",
    "print(f\"\\nDataset Summary:\")\n",
    "print(f\"   Kaggle train: {len(df_train)} articles\")\n",
    "print(f\"   Kaggle test: {len(df_test)} articles\")\n",
    "if df_politifact is not None:\n",
    "    print(f\"   PolitiFact: {len(df_politifact)} articles\")\n",
    "if df_gossipcop is not None:\n",
    "    print(f\"   GossipCop: {len(df_gossipcop)} articles\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "53ccfbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenize(text: str):\n",
    "    # keep it simple and robust\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    return text.split()\n",
    "\n",
    "# ---------- vocab builder ----------\n",
    "PAD, UNK = \"<pad>\", \"<unk>\"\n",
    "\n",
    "def build_vocab(texts, max_vocab=80000, min_freq=2):\n",
    "    counter = Counter()\n",
    "    for t in texts:\n",
    "        counter.update(simple_tokenize(t))\n",
    "\n",
    "    vocab = {PAD: 0, UNK: 1}\n",
    "    for tok, freq in counter.most_common():\n",
    "        if freq < min_freq:\n",
    "            break\n",
    "        if tok not in vocab:\n",
    "            vocab[tok] = len(vocab)\n",
    "        if len(vocab) >= max_vocab:\n",
    "            break\n",
    "    return vocab\n",
    "\n",
    "def encode(text, vocab, max_len=512):\n",
    "    toks = simple_tokenize(text)\n",
    "    ids = [vocab.get(tok, vocab[UNK]) for tok in toks[:max_len]]\n",
    "    # pad\n",
    "    if len(ids) < max_len:\n",
    "        ids += [vocab[PAD]] * (max_len - len(ids))\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c9aa790",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df, vocab, max_len=512):\n",
    "        self.texts = df[\"text\"].tolist()\n",
    "        self.labels = df[\"label\"].astype(int).tolist()\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(encode(self.texts[idx], self.vocab, self.max_len), dtype=torch.long)\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5d63b2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=128, hidden_dim=128, num_layers=1, bidirectional=True, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=emb_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "        )\n",
    "\n",
    "        out_dim = hidden_dim * (2 if bidirectional else 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(out_dim, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = (x != 0).float()          # (B, T)\n",
    "        emb = self.embedding(x)          # (B, T, E)\n",
    "        out, _ = self.lstm(emb)          # (B, T, 2H)\n",
    "\n",
    "        mask = mask.unsqueeze(-1)        # (B, T, 1)\n",
    "        out = out * mask\n",
    "        pooled = out.sum(dim=1) / mask.sum(dim=1).clamp(min=1.0)  # (B, 2H)\n",
    "\n",
    "        pooled = self.dropout(pooled)\n",
    "        return self.fc(pooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3233ee68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    progress = tqdm(loader, desc=\"Training\", leave=False)\n",
    "\n",
    "    for x, y in progress:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.extend(preds.detach().cpu().numpy())\n",
    "        all_labels.extend(y.detach().cpu().numpy())\n",
    "\n",
    "        progress.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    return (\n",
    "        np.mean(losses),\n",
    "        accuracy_score(all_labels, all_preds),\n",
    "        f1_score(all_labels, all_preds, average=\"macro\"),\n",
    "        precision_score(all_labels, all_preds, average=\"macro\", zero_division=0),\n",
    "    )\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_model(model, loader, criterion, device, desc=\"Evaluating\"):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    for x, y in tqdm(loader, desc=desc):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    return (\n",
    "        np.mean(losses),\n",
    "        accuracy_score(all_labels, all_preds),\n",
    "        f1_score(all_labels, all_preds, average=\"macro\"),\n",
    "        precision_score(all_labels, all_preds, average=\"macro\", zero_division=0),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c2f34634",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss=0.0749, f1=0.9706, accuracy=0.9709, precision=0.9700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: loss=0.0090, f1=0.9973, accuracy=0.9973, precision=0.9973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: loss=0.0064, f1=0.9979, accuracy=0.9979, precision=0.9979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "#` ---------- training loop ----------\n",
    "epochs = 3\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "vocab = build_vocab(df_train[\"text\"])\n",
    "train_ds = TextDataset(df_train, vocab)\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "\n",
    "model = LSTMClassifier(vocab_size=len(vocab)).to(device)\n",
    "counts = df_train[\"label\"].value_counts().sort_index()\n",
    "weights = torch.tensor([1.0/counts[0], 1.0/counts[1]], dtype=torch.float, device=device)\n",
    "weights = weights / weights.sum() * 2\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=7e-4, weight_decay=1e-2)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc, train_f1, train_prec = train_one_epoch(\n",
    "        model, train_loader, optimizer, criterion, device\n",
    "    )\n",
    "    print(f\"Epoch {epoch+1}: loss={train_loss:.4f}, f1={train_f1:.4f}, accuracy={train_acc:.4f}, precision={train_prec:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8ebe8a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "torch.save(model.state_dict(), \"lstm_model.pt\")\n",
    "\n",
    "with open(\"vocab.json\", \"w\") as f:\n",
    "    json.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "73f2549f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 242/242 [00:27<00:00,  8.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle Test Results | Loss: 0.0286 | Acc: 0.9902 | F1: 0.9901 | Prec: 0.9894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 20/20 [00:02<00:00,  9.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PolitiFact Results  | Loss: 4.5474 | Acc: 0.5128 | F1: 0.3612 | Prec: 0.5882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 455/455 [00:52<00:00,  8.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GossipCop Results   | Loss: 9.5921 | Acc: 0.2346 | F1: 0.1921 | Prec: 0.4709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load vocab\n",
    "with open(\"vocab.json\", \"r\") as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "# Load model\n",
    "model = LSTMClassifier(vocab_size=len(vocab)).to(device)\n",
    "model.load_state_dict(torch.load(\"lstm_model.pt\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Evaluate on Kaggle test set\n",
    "\n",
    "kaggle_test_ds = TextDataset(df_test, vocab)\n",
    "kaggle_test_loader = DataLoader(\n",
    "    kaggle_test_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "k_loss, k_acc, k_f1, k_prec = eval_model(\n",
    "    model,\n",
    "    kaggle_test_loader,\n",
    "    criterion,\n",
    "    device\n",
    ")\n",
    "\n",
    "print(f\"Kaggle Test Results | Loss: {k_loss:.4f} | Acc: {k_acc:.4f} | F1: {k_f1:.4f} | Prec: {k_prec:.4f}\")\n",
    "\n",
    "# Evaluate on PolitiFact\n",
    "politifact_ds = TextDataset(df_politifact, vocab)\n",
    "politifact_loader = DataLoader(\n",
    "    politifact_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "p_loss, p_acc, p_f1, p_prec = eval_model(\n",
    "    model,\n",
    "    politifact_loader,\n",
    "    criterion,\n",
    "    device\n",
    ")\n",
    "print(f\"PolitiFact Results  | Loss: {p_loss:.4f} | Acc: {p_acc:.4f} | F1: {p_f1:.4f} | Prec: {p_prec:.4f}\")\n",
    "\n",
    "# Evaluate on GossipCop\n",
    "gossipcop_ds = TextDataset(df_gossipcop, vocab)\n",
    "gossipcop_loader = DataLoader(\n",
    "    gossipcop_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "g_loss, g_acc, g_f1, g_prec = eval_model(\n",
    "    model,\n",
    "    gossipcop_loader,\n",
    "    criterion,\n",
    "    device\n",
    ")\n",
    "print(f\"GossipCop Results   | Loss: {g_loss:.4f} | Acc: {g_acc:.4f} | F1: {g_f1:.4f} | Prec: {g_prec:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4622e1b5",
   "metadata": {},
   "source": [
    "## Mixed Dataset Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b45bbe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 87/87 [00:06<00:00, 12.49it/s]             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | train f1=0.8568 | val f1=0.9079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 87/87 [00:06<00:00, 12.53it/s]             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | train f1=0.9367 | val f1=0.9311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 87/87 [00:07<00:00, 12.30it/s]             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | train f1=0.9662 | val f1=0.9244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "kaggle_sample = df_train.sample(n=20000, random_state=42)\n",
    "politifact_sample = df_politifact.sample(frac=0.5, random_state=42)\n",
    "gossipcop_sample = df_gossipcop.sample(frac=0.5, random_state=42)\n",
    "\n",
    "# Build mixed dataframe like you did\n",
    "df_mixed = pd.concat([kaggle_sample, politifact_sample, gossipcop_sample], ignore_index=True)\n",
    "df_mixed = df_mixed.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split mixed into train/val\n",
    "df_mixed_train, df_mixed_val = train_test_split(\n",
    "    df_mixed, test_size=0.1, random_state=42, stratify=df_mixed[\"label\"]\n",
    ")\n",
    "\n",
    "# Build vocab from mixed training text (important)\n",
    "vocab = build_vocab(df_mixed_train[\"text\"], max_vocab=80000, min_freq=2)\n",
    "\n",
    "train_loader = DataLoader(TextDataset(df_mixed_train, vocab, max_len=512), batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(TextDataset(df_mixed_val, vocab, max_len=512), batch_size=32, shuffle=False)\n",
    "\n",
    "model = LSTMClassifier(vocab_size=len(vocab)).to(device)\n",
    "\n",
    "# (Recommended) class weights\n",
    "counts = df_mixed_train[\"label\"].value_counts().sort_index()\n",
    "weights = torch.tensor([1.0/counts[0], 1.0/counts[1]], dtype=torch.float, device=device)\n",
    "weights = weights / weights.sum() * 2\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-3)\n",
    "\n",
    "for ep in range(3):\n",
    "    tr = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    va = eval_model(model, val_loader, criterion, device)\n",
    "    print(f\"Epoch {ep+1} | train f1={tr[2]:.4f} | val f1={va[2]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56ef8b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout sizes:\n",
      "  Kaggle holdout: 7729\n",
      "  PolitiFact holdout: 312\n",
      "  GossipCop holdout: 7275\n"
     ]
    }
   ],
   "source": [
    "df_kaggle_holdout = df_test.copy()\n",
    "\n",
    "# PolitiFact: hold out the rows NOT included in politifact_sample\n",
    "politifact_holdout = df_politifact.drop(index=politifact_sample.index).reset_index(drop=True)\n",
    "\n",
    "# GossipCop: hold out the rows NOT included in gossipcop_sample\n",
    "gossipcop_holdout = df_gossipcop.drop(index=gossipcop_sample.index).reset_index(drop=True)\n",
    "\n",
    "print(\"Holdout sizes:\")\n",
    "print(\"  Kaggle holdout:\", len(df_kaggle_holdout))\n",
    "print(\"  PolitiFact holdout:\", len(politifact_holdout))\n",
    "print(\"  GossipCop holdout:\", len(gossipcop_holdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7394f9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "max_len = 512\n",
    "\n",
    "kaggle_holdout_loader = DataLoader(\n",
    "    TextDataset(df_kaggle_holdout, vocab, max_len=max_len),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "politifact_holdout_loader = DataLoader(\n",
    "    TextDataset(politifact_holdout, vocab, max_len=max_len),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "gossipcop_holdout_loader = DataLoader(\n",
    "    TextDataset(gossipcop_holdout, vocab, max_len=max_len),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d93a846",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 242/242 [00:28<00:00,  8.61it/s]\n",
      "Evaluating: 100%|██████████| 10/10 [00:01<00:00,  8.39it/s]\n",
      "Evaluating: 100%|██████████| 228/228 [00:27<00:00,  8.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle Holdout     | Loss: 0.0386 | Acc: 0.9887 | F1: 0.9886 | Prec: 0.9897\n",
      "PolitiFact Holdout | Loss: 1.0348 | Acc: 0.6154 | F1: 0.6044 | Prec: 0.6522\n",
      "GossipCop Holdout  | Loss: 0.6264 | Acc: 0.7737 | F1: 0.6922 | Prec: 0.6887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "k_loss, k_acc, k_f1, k_prec = eval_model(model, kaggle_holdout_loader, criterion, device)\n",
    "p_loss, p_acc, p_f1, p_prec = eval_model(model, politifact_holdout_loader, criterion, device)\n",
    "g_loss, g_acc, g_f1, g_prec = eval_model(model, gossipcop_holdout_loader, criterion, device)\n",
    "\n",
    "print(f\"Kaggle Holdout     | Loss: {k_loss:.4f} | Acc: {k_acc:.4f} | F1: {k_f1:.4f} | Prec: {k_prec:.4f}\")\n",
    "print(f\"PolitiFact Holdout | Loss: {p_loss:.4f} | Acc: {p_acc:.4f} | F1: {p_f1:.4f} | Prec: {p_prec:.4f}\")\n",
    "print(f\"GossipCop Holdout  | Loss: {g_loss:.4f} | Acc: {g_acc:.4f} | F1: {g_f1:.4f} | Prec: {g_prec:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12860a08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
